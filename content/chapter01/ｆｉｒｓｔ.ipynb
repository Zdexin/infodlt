{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\envs\\tensorFlow\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "np.random.seed(2018)\n",
    "import os \n",
    "import glob \n",
    "import cv2 \n",
    "import datetime\n",
    "import pandas as pd \n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.cross_validation import KFold \n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution2D,MaxPooling2D,ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping \n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import log_loss\n",
    "from keras import  __version__  as keras_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# ––––––––––\n",
    "# img_path : path\n",
    "#\tpath of the image to be resized \n",
    "def rezize_img(img_path):\n",
    "    #reading image file\n",
    "    img = cv2.imread(img_path)\n",
    "    #Resize the image to to be 32 by 32\n",
    "    img_resized = cv2.resize(img, (32, 32), cv2.INTER_LINEAR)\n",
    "    return img_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the training samples and their corresponding labels \n",
    "def load_training_samples():\n",
    "#Variables to hold the training input and output variables \n",
    "    train_input_variables = []\n",
    "    train_input_variables_id = [] \n",
    "    train_label = []\n",
    "    # Scanning all images in each folder of a fish type \n",
    "    print('Start Reading Train Images')\n",
    "    folders = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "    for fld in folders:\n",
    "        folder_index = folders.index(fld)\n",
    "        print('Load folder (} (Index: (})'.format(fld, folder_index)) \n",
    "        imgs_path = os.path.join('..', 'input', 'train', fld, '*.jpg') \n",
    "        files = glob.glob(imgs_path)\n",
    "        for file in files:\n",
    "            file_base = os.path.basename(file)\n",
    "            # Resize the image\n",
    "            resized_img = rezize_image(file)\n",
    "            # Appending the processed image to the input/output variables of the classifier\n",
    "            train_input_variables.append(resized_img) \n",
    "            train_input_variables_id.append(file_base) \n",
    "            train_label.append(folder_index)\n",
    "        return train_input_variables, train_input_variables_id, train_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the testing samples which will be used to testing how well the model was trained\n",
    "def load_testing_samples():\n",
    "# 3canning images from the test folder\n",
    "    imgs_path = os.path.join('..', 'input', 'test_stgl', '*.jpg')\n",
    "    files = sorted(glob.glob(imgs_path))\n",
    "    # Variables to hold the testing samples \n",
    "    testing_samples = []\n",
    "    testing_samples_id = []\n",
    "    #Processing the images and appending them to the array that we have \n",
    "    for file in files:\n",
    "        file_base = os.path.basename(file)\n",
    "        # Image resizing\n",
    "        resized_img = rezize_image(file) \n",
    "        testing_samples.append(resized_img) \n",
    "        testing_samples_id.append(file_base)\n",
    "    return testing_samples, testing_samples_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting the images to fit our model\n",
    "def format_results_for_types(predictions, test_id, info):\n",
    "    model_results = pd.DataFrame(predictions, columns=['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER','SHARK', 'YFT'])\n",
    "    model_results.loc[:, 'image'] = pd.Series(test_id, index=model_results.index)\n",
    "    sub_file = 'testOutput_' + info + '.csv' \n",
    "    model_results.to_csv(sub_file, index=False)\n",
    "def load_normalize_training_samples():\n",
    "    # Calling the load function in order to load and resize the training samples\n",
    "    training_samples, training_label, training_samples_id = load_training_samples()\n",
    "    # Converting the loaded and resized data into Numpy format \n",
    "    training_samples = np.array(training_samples, dtype=np.uint8) \n",
    "    training_label = np.array(training_label, dtype=np.uint8)\n",
    "    # Reshaping the training samples\n",
    "    training_samples = training_samples.transpose((0,3,1,2))\n",
    "    # Converting the training samples and training labels into float format \n",
    "    training_samples = training_samples.astype('float32')\n",
    "    training_samples = training_samples/255\n",
    "    training_label = np_utils.to_categorical(training_label, 8) \n",
    "    return training_samples, training_label, training_samples_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading and normalizing the testing sample to fit into our model \n",
    "def load_normalize_testing_samples():\n",
    "    # Calling the load function in order to load and resize the testing samples\n",
    "    testing_samples, testing_samples_id = load_testing_samples()\n",
    "    # Converting the loaded and resized data into Numpy format \n",
    "    testing_samples = np.array(testing_samples, dtype=np.uint8)\n",
    "    # Reshaping the testing samples\n",
    "    testing_samples = testing_samples.transpose((0,3,1,2))\n",
    "    # Converting the testing samples into float format \n",
    "    testing_samples = testing_samples.astype('float32') \n",
    "    testing_samples = testing_samples / 255\n",
    "    return testing_samples, testing_samples_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_several_folds_mean(data, num_folds):\n",
    "    a = np.array(data[O])\n",
    "    for i in range(l, num_folds): \n",
    "        a += np.array(data[i])\n",
    "    a /= num_folds \n",
    "    return a.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CNN model architecture \n",
    "def create_cnn_model_arch():\n",
    "    pool_size = 2 # we will use 2x2 pooling throughout\n",
    "    conv_depth_l = 32 # we will initially have 32 kernels per conv. layer...\n",
    "    conv_depth_2 = 64 # ...switching to 64 after the first pooling layer kernel_size = 3 # we will use 3x3 kernels throughout\n",
    "    drop_prob = 0.5 # dropout in the FC layer with probability O.5 \n",
    "    hidden_size = 32 # the FC layer will have 5l2 neurons \n",
    "    num_classes = 8 # there are 8 fish types\n",
    "    # Conv [32] –> Conv [32] –> Pool \n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(ZeroPadding2D((l, l), input_shape=(3, 32, 32), dim_ordering='th'))\n",
    "    cnn_model.add(Convolution2D(conv_depth_l, kernel_size, kernel_size, activation='relu',\n",
    "    dim_ordering='th'))\n",
    "    cnn_model.add(ZeroPadding2D((l, l), dim_ordering='th')) \n",
    "    cnn_model.add(Convolution2D(conv_depth_l, kernel_size, kernel_size,activation='relu', dim_ordering='th'))\n",
    "    cnn_model.add(MaxPooling2D(pool_size=(pool_size, pool_size), strides=(2, 2),dim_ordering='th'))\n",
    "    # Conv [64] –> Conv [64] –> Pool \n",
    "    cnn_model.add(ZeroPadding2D((l, l), dim_ordering='th'))\n",
    "    cnn_model.add(Convolution2D(conv_depth_2, kernel_size, kernel_size, activation='relu',dim_ordering='th'))\n",
    "    cnn_model.add(ZeroPadding2D((l, l), dim_ordering='th')) \n",
    "    cnn_model.add(Convolution2D(conv_depth_2, kernel_size, kernel_size,activation='relu',dim_ordering='th')) \n",
    "    cnn_model.add(MaxPooling2D(pool_size=(pool_size, pool_size),strides=(2, 2),dim_ordering='th'))\n",
    "    # Now flatten to lD, apply FC then ReLU (with dropout) and finally softmax(output layer)\n",
    "    cnn_model.add(Flatten()) \n",
    "    cnn_model.add(Dense(hidden_size, activation='relu')) \n",
    "    cnn_model.add(Dropout(drop_prob)) \n",
    "    cnn_model.add(Dense(hidden_size, activation='relu')) \n",
    "    cnn_model.add(Dropout(drop_prob)) \n",
    "    cnn_model.add(Dense(num_classes, activation='softmax'))\n",
    "    # initiating the stochastic gradient descent optimiser \n",
    "    stochastic_gradient_descent = SGD(lr=le-2, decay=le-6, momentum=0.9,nesterov=True)\n",
    "    cnn_model.compile(optimizer=stochastic_gradient_descent,\n",
    "    # using the stochastic gradient descent optimiser\n",
    "    loss='categorical_crossentropy')# using the cross– entropy loss function\n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model using with kfold cross validation as a validation method \n",
    "def create_model_with_kfold_cross_validation(nfolds=10):\n",
    "    batch_size = 16 # in each iteration, we consider 32 training examples at once\n",
    "    num_epochs = 30 # we iterate 2OO times over the entire training set \n",
    "    random_state =51 # control the randomness for reproducibility of the results on the same platform\n",
    "    # Loading and normalizing the training samples prior to feeding it to the created CNN model\n",
    "    training_samples, training_samples_target, training_samples_id =load_normalize_training_samples() \n",
    "    yfull_train = dict()\n",
    "    # Providing Training/Testing indices to split data in the training samples\n",
    "    # which is splitting data into lO consecutive folds with shuffling \n",
    "    kf = KFold(len(train_id), n_folds=nfolds, shuffle=True,random_state=random_state)\n",
    "    fold_number = 0 # Initial value for fold number\n",
    "    sum_score = 0 # overall score (will be incremented at each iteration) \n",
    "    trained_models = [] # storing the modeling of each iteration over the folds\n",
    "    # Getting the training/testing samples based on the generated \n",
    "    #training/testing indices by Kfold\n",
    "    for train_index,test_index in kf: \n",
    "        cnn_model = create_cnn_model_arch()\n",
    "        training_samples_X = training_samples[train_index] # Getting the training input variables\n",
    "        training_samples_Y = training_samples_target[train_index] # Getting the training output/label variable\n",
    "        validation_samples_X = training_samples[test_index] # Getting the validation input variables\n",
    "        validation_samples_Y = training_samples_target[test_index] # Getting the validation output/label variable\n",
    "        fold_number += 1\n",
    "        print('Fold number {} from {}'.format(fold_number, nfolds)) \n",
    "        callbacks = [\n",
    "                EarlyStopping(monitor='val_loss', patience=3, verbose=0),\n",
    "        ]\n",
    "        # Fitting the CNN model giving the defined settings \n",
    "        cnn_model.fit(training_samples_X, training_samples_Y,batch_size=batch_size,\n",
    "        nb_epoch=num_epochs, shuffle=True, verbose=2,\n",
    "        validation_data=(validation_samples_X,\n",
    "        validation_samples_Y), callbacks=callbacks)\n",
    "        # measuring the generalization ability of the trained model based on the validation set\n",
    "        predictions_of_validation_samples = cnn_model.predict(validation_samples_X.astype('float32'), batch_size=batch_size, verbose=2)\n",
    "        current_model_score = log_loss(Y_valid, predictions_of_validation_samples)\n",
    "        print('Current model score log_loss: ', current_model_score) \n",
    "        sum_score += current_model_score*len(test_index)\n",
    "        # Store valid predictions\n",
    "        for i in range(len(test_index)):\n",
    "            yfull_train[test_index[i]] = predictions_of_validation_samples[i]\n",
    "            # Store the trained model \n",
    "            trained_models.append(cnn_model)\n",
    "            # incrementing the sum_score value by the current model calculated score\n",
    "        overall_score = sum_score/len(training_samples) \n",
    "        print(\"Log_loss train independent avg: \", overall_score)\n",
    "        #Reporting the model loss at this stage \n",
    "        overall_settings_output_string = 'loss_' + str(overall_score) +'_folds_' + str(nfolds) + '_ep_' + str(num_epochs)\n",
    "        return overall_settings_output_string, trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing how well the model is trained \n",
    "def test_generality_crossValidation_over_test_set( overall_settings_output_string, cnn_models):\n",
    "    batch_size = 16 # in each iteration, we consider 32 training examples at once\n",
    "    fold_number = 0 # fold iterator\n",
    "    number_of_folds = len(cnn_models) # Creating number of folds based on the value used in the training step\n",
    "    yfull_test = [] # variable to hold overall predictions for the test set\n",
    "    #executing the actual cross validation test process over the test set \n",
    "    for j in range(number_of_folds):\n",
    "        model = cnn_models[j] \n",
    "        fold_number += 1\n",
    "        print('Fold number {} out of {}'.format(fold_number, number_of_folds))\n",
    "        #Loading and normalizing testing samples \n",
    "        testing_samples, testing_samples_id =load_normalize_testing_samples()\n",
    "        #Calling the current model over the current test fold \n",
    "        test_prediction = model.predict(testing_samples,batch_size=batch_size, verbose=2) \n",
    "        yfull_test.append(test_prediction)\n",
    "    test_result = merge_several_folds_mean(yfull_test, number_of_folds)\n",
    "    overall_settings_output_string = 'loss_' +overall_settings_output_string \\\n",
    "    + '_folds_' + str(number_of_folds)\n",
    "    format_results_for_types(test_result, testing_samples_id, overall_settings_output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the model training and testing\n",
    "if __name__== '_main_':\n",
    "    info_string, models = create_model_with_kfold_cross_validation() \n",
    "    test_generality_crossValidation_over_test_set(info_string, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
