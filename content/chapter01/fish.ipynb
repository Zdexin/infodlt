{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以TensorFlow为后端，在深度学习平台Keras中加载所需的库\n",
    "import numpy as np \n",
    "np.random.seed(2018)\n",
    "import os \n",
    "import glob \n",
    "import cv2 \n",
    "import datetime\n",
    "import pandas as pd \n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.cross_validation import KFold \n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution2D,MaxPooling2D,ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping \n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import log_loss\n",
    "from keras import  __version__  as keras_version\n",
    "# Parameters\n",
    "# ––––––––––\n",
    "# x : type\n",
    "#\tDescription of parameter `x`. \n",
    "def rezize_image(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    img_resized = cv2.resize(img, (32, 32), cv2.INTER_LINEAR) \n",
    "    return img_resized\n",
    "#从相应的文件夹名称加载培训示例，其中为每种类型都有一个文件夹\n",
    "def load_training_samples():\n",
    "#用于保存培训输入和输出变量的变量\n",
    "    train_input_variables = []\n",
    "    train_input_variables_id = []\n",
    "    train_label = []\n",
    "    # 扫描鱼类型的每个文件夹中的所有图像\n",
    "    print('Start Reading Train Images')\n",
    "    folders = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "    for fld in folders:\n",
    "        folder_index = folders.index(fld)\n",
    "        print('Load folder (} (Index: (})'.format(fld, folder_index)) \n",
    "        imgs_path = os.path.join('..', 'input', 'train', fld, '*.jpg') \n",
    "        files = glob.glob(imgs_path)\n",
    "        for file in files:\n",
    "            file_base = os.path.basename(file)\n",
    "            # 调整图像大小\n",
    "            resized_img = rezize_image(file)\n",
    "            # 将处理后的图像附加到分类器的输入/输出变量中\n",
    "            train_input_variables.append(resized_img) \n",
    "            train_input_variables_id.append(file_base) \n",
    "            train_label.append(folder_index)\n",
    "        return train_input_variables, train_input_variables_id, train_label\n",
    "\n",
    "#加载测试样本，用于测试模型的训练效果。\n",
    "def load_testing_samples():\n",
    "# 从测试文件夹中扫描图像\n",
    "    imgs_path = os.path.join('..', 'input', 'test_stgl', '*.jpg')\n",
    "    files = sorted(glob.glob(imgs_path))\n",
    "    # 保存测试样本的变量 \n",
    "    testing_samples = []\n",
    "    testing_samples_id = []\n",
    "    #处理图像并将它们附加到我们拥有的数组中。 \n",
    "    for file in files:\n",
    "        file_base = os.path.basename(file)\n",
    "        # Image resizing\n",
    "        resized_img = rezize_image(file) \n",
    "        testing_samples.append(resized_img) \n",
    "        testing_samples_id.append(file_base)\n",
    "    return testing_samples, testing_samples_id\n",
    "# 格式化图像以适应我们的模型\n",
    "def format_results_for_types(predictions, test_id, info):\n",
    "    model_results = pd.DataFrame(predictions, columns=['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER','SHARK', 'YFT'])\n",
    "    model_results.loc[:, 'image'] = pd.Series(test_id, index=model_results.index)\n",
    "    sub_file = 'testOutput_' + info + '.csv' \n",
    "    model_results.to_csv(sub_file, index=False)\n",
    "def load_normalize_training_samples():\n",
    "    # 调用Load函数以加载和调整训练样本的大小\n",
    "    training_samples, training_label, training_samples_id = load_training_samples()\n",
    "    # 将加载和调整大小的数据转换为Numpy格式 \n",
    "    training_samples = np.array(training_samples, dtype=np.uint8) \n",
    "    training_label = np.array(training_label, dtype=np.uint8)\n",
    "    # 重塑训练样本\n",
    "    training_samples = training_samples.transpose((0,3,1,2))\n",
    "    # 将培训样本和培训标签转换为浮动格式\n",
    "    training_samples = training_samples.astype('float32')\n",
    "    training_samples = training_samples/255\n",
    "    training_label = np_utils.to_categorical(training_label, 8) \n",
    "    return training_samples, training_label, training_samples_id\n",
    "#加载和规范化测试样本以适应我们的模型\n",
    "def load_normalize_testing_samples():\n",
    "    # 调用LOAD函数以加载和调整测试样本的大小\n",
    "    testing_samples, testing_samples_id = load_testing_samples()\n",
    "    # 将加载和调整大小的数据转换为Numpy格式\n",
    "    testing_samples = np.array(testing_samples, dtype=np.uint8)\n",
    "    # 重塑测试样本\n",
    "    testing_samples = testing_samples.transpose((0,3,1,2))\n",
    "    # 将测试样本转换为浮动格式\n",
    "    testing_samples = testing_samples.astype('float32') \n",
    "    testing_samples = testing_samples / 255\n",
    "    return testing_samples, testing_samples_id\n",
    "\n",
    "def merge_several_folds_mean(data, num_folds):\n",
    "    a = np.array(data[O])\n",
    "    for i in range(l, num_folds): \n",
    "        a += np.array(data[i])\n",
    "    a /= num_folds \n",
    "    return a.tolist()\n",
    "# 创建CNN模型体系结构\n",
    "def create_cnn_model_arch():\n",
    "    pool_size = 2 # 我们将在整个过程中使用2x2池化层\n",
    "    conv_depth_l = 32 \n",
    "    conv_depth_2 = 64\n",
    "    drop_prob = 0.5  \n",
    "    hidden_size = 32 \n",
    "    num_classes = 8 \n",
    "    # Conv [32] –> Conv [32] –> Pool \n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(ZeroPadding2D((l, l), input_shape=(3, 32, 32), dim_ordering='th'))\n",
    "    cnn_model.add(Convolution2D(conv_depth_l, kernel_size, kernel_size, activation='relu',\n",
    "    dim_ordering='th'))\n",
    "    cnn_model.add(ZeroPadding2D((l, l), dim_ordering='th')) \n",
    "    cnn_model.add(Convolution2D(conv_depth_l, kernel_size, kernel_size,activation='relu', dim_ordering='th'))\n",
    "    cnn_model.add(MaxPooling2D(pool_size=(pool_size, pool_size), strides=(2, 2),dim_ordering='th'))\n",
    "    # Conv [64] –> Conv [64] –> Pool \n",
    "    cnn_model.add(ZeroPadding2D((l, l), dim_ordering='th'))\n",
    "    cnn_model.add(Convolution2D(conv_depth_2, kernel_size, kernel_size, activation='relu',dim_ordering='th'))\n",
    "    cnn_model.add(ZeroPadding2D((l, l), dim_ordering='th')) \n",
    "    cnn_model.add(Convolution2D(conv_depth_2, kernel_size, kernel_size,activation='relu',dim_ordering='th')) \n",
    "    cnn_model.add(MaxPooling2D(pool_size=(pool_size, pool_size),strides=(2, 2),dim_ordering='th'))\n",
    "    # Now flatten to lD, apply FC then ReLU (with dropout) and finally softmax(output layer)\n",
    "    cnn_model.add(Flatten()) \n",
    "    cnn_model.add(Dense(hidden_size, activation='relu')) \n",
    "    cnn_model.add(Dropout(drop_prob)) \n",
    "    cnn_model.add(Dense(hidden_size, activation='relu')) \n",
    "    cnn_model.add(Dropout(drop_prob)) \n",
    "    cnn_model.add(Dense(num_classes, activation='softmax'))\n",
    "    # 启动随机梯度下降优化器\n",
    "    stochastic_gradient_descent = SGD(lr=le-2, decay=le-6, momentum=0.9,nesterov=True)\n",
    "    cnn_model.compile(optimizer=stochastic_gradient_descent,\n",
    "    # 使用随机梯度下降优化器\n",
    "    loss='categorical_crossentropy')# 使用交叉熵损失函数\n",
    "    return cnn_model\n",
    "#以折叠交叉验证为验证方法的模型 \n",
    "def create_model_with_kfold_cross_validation(nfolds=10):\n",
    "    batch_size = 16 # 在每次迭代中，我们同时考虑32个训练示例。\n",
    "    num_epochs = 30 # 我们在整个训练集上迭代2OO次。\n",
    "    random_state =51 # 在同一平台上控制结果重复性的随机性\n",
    "    # 在将训练样本输入到创建的CNN模型之前加载和规范化\n",
    "    training_samples, training_samples_target, training_samples_id =load_normalize_training_samples() \n",
    "    yfull_train = dict()\n",
    "    # 提供培训/测试指标，以分割培训样本中的数据\n",
    "    # which is splitting data into lO consecutive folds with shuffling \n",
    "    kf = KFold(len(train_id), n_folds=nfolds, shuffle=True,random_state=random_state)\n",
    "    fold_number = 0 # 折数初值\n",
    "    sum_score = 0 #总分(每次迭代时将增加)\n",
    "    trained_models = [] # 存储每个迭代的模型\n",
    "    # 获取培训/测试样本\n",
    "    #t培训/测试指数\n",
    "    for train_index,test_index in kf: \n",
    "        cnn_model = create_cnn_model_arch()\n",
    "        training_samples_X = training_samples[train_index] # 获取训练输入变量\n",
    "        training_samples_Y = training_samples_target[train_index] # 获取培训输出/标签变量\n",
    "        validation_samples_X = training_samples[test_index] # 获取验证输入变量\n",
    "        validation_samples_Y = training_samples_target[test_index] # 获取验证输出/标签变量\n",
    "        fold_number += 1\n",
    "        print('Fold number {} from {}'.format(fold_number, nfolds)) \n",
    "        callbacks = [\n",
    "                EarlyStopping(monitor='val_loss', patience=3, verbose=0),\n",
    "        ]\n",
    "        # 拟合CNN模型，给出定义的设置\n",
    "        cnn_model.fit(training_samples_X, training_samples_Y,batch_size=batch_size,\n",
    "        nb_epoch=num_epochs, shuffle=True, verbose=2,\n",
    "        validation_data=(validation_samples_X,\n",
    "        validation_samples_Y), callbacks=callbacks)\n",
    "        # 基于验证集的训练模型泛化能力度量\n",
    "        predictions_of_validation_samples = cnn_model.predict(validation_samples_X.astype('float32'), batch_size=batch_size, verbose=2)\n",
    "        current_model_score = log_loss(Y_valid, predictions_of_validation_samples)\n",
    "        print('Current model score log_loss: ', current_model_score) \n",
    "        sum_score += current_model_score*len(test_index)\n",
    "        # 存储有效预测\n",
    "        for i in range(len(test_index)):\n",
    "            yfull_train[test_index[i]] = predictions_of_validation_samples[i]\n",
    "            # 存储经过训练的模型\n",
    "            trained_models.append(cnn_model)\n",
    "            # 用当前模型计算的分数增量和得分值\n",
    "        overall_score = sum_score/len(training_samples) \n",
    "        print(\"Log_loss train independent avg: \", overall_score)\n",
    "        #在此阶段报告模型损失\n",
    "        overall_settings_output_string = 'loss_' + str(overall_score) +'_folds_' + str(nfolds) + '_ep_' + str(num_epochs)\n",
    "        return overall_settings_output_string, trained_models\n",
    "#测试模型的训练效果\n",
    "def test_generality_crossValidation_over_test_set( overall_settings_output_string, cnn_models):\n",
    "    batch_size = 16 # 在每次迭代中，我们同时考虑32个训练示例。\n",
    "    fold_number = 0 # 折叠迭代器\n",
    "    number_of_folds = len(cnn_models) # 根据训练步骤中使用的值创建折叠数\n",
    "    yfull_test = [] # 变量来保存测试集的总体预测。\n",
    "    #在测试集上执行实际的交叉验证测试过程 \n",
    "    for j in range(number_of_folds):\n",
    "        model = cnn_models[j] \n",
    "        fold_number += 1\n",
    "        print('Fold number {} out of {}'.format(fold_number, number_of_folds))\n",
    "        #加载和正规化测试样本\n",
    "        testing_samples, testing_samples_id =load_normalize_testing_samples()\n",
    "        #在当前测试折叠上调用当前模型\n",
    "        test_prediction = model.predict(testing_samples,batch_size=batch_size, verbose=2) \n",
    "        yfull_test.append(test_prediction)\n",
    "    test_result = merge_several_folds_mean(yfull_test, number_of_folds)\n",
    "    overall_settings_output_string = 'loss_' +overall_settings_output_string \\\n",
    "    + '_folds_' + str(number_of_folds)\n",
    "    format_results_for_types(test_result, testing_samples_id, overall_settings_output_string)\n",
    "#开始模型培训和测试\n",
    "if __name__== '_main_':\n",
    "    info_string, models = create_model_with_kfold_cross_validation() \n",
    "    test_generality_crossValidation_over_test_set(info_string, models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
