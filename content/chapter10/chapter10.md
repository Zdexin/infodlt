# 第10章 递归神经网络-语言建模
&emsp;&emsp; 循环神经网络（RNN）是一种广泛用于自然语言处理的深层学习结构。这组 结构使我们能够为当前预测提供相关信息，并且还具有处理任何输入序列中的长期依赖关系的特定结构。在本章中，我们将演示如何构造序列到如何构造序列模型，这将在NLP中的许多应用中都有用。我们将通过构建字符型语言模型来演示这些概念，并查看我们的模型如何生成与原始输入序列类似的语句。
本章将讨论以下主题：
#### •递归神经网络背后的感知
#### •LSTM（长短期记忆神经网络）网络
#### •语言模型的实现
## 递归神经网络背后的感知
&emsp;&emsp; 到目前为止，我们所处理的所有深度学习结构都没有机制来记忆它们以前收到的输入。例如，如果给前馈神经网络（FNN）输入一系列字符，例如HELLO，当输入到达E时，您会发现它没有保存任何信息即忘记它只读取H。这是基于序列学习的严重问题。而且由于它没有以前读过的字符的记忆，这种神经网络很难通过训练来预测下一个字符。这对于语言建模、机器翻译、语音识别等许多应用都没有意义。<br>
&emsp;&emsp; 由于这个特定的原因，我们将介绍RNNs（递归神经网络），一组深层学习体系结构，它们确实保存了信息并记住了它刚刚遇到的内容。让我们演示一下RNNS应该如何处理相同的输入序列。<br>
&emsp;&emsp; 字符，HELLO。当RNN信元/单元接收E作为输入时，它也接收较早接收到的字符H。将当前字符和过去字符作为输入提供给RNN单元为这些体系结构（即短期内存）提供了很大的作用；它还使得这些体系结构可用于预测/猜测H之后最有可能的字符（即L），在这个特定的序列中可能具体字母。<br>
&emsp;&emsp; 我们已经看到，以前的体系结构为它们的输入分配权重；RNNS遵循相同的优化过程，为它们的多个输入分配权重，这就是现在和过去。因此，在这种情况下，神经网络将给它们中的每一个输入分配两个不同的权重矩阵。为了做到这一点，我们将使用梯度下降和较重的反向传播（BPTT）方法。
## 递归神经网络的结构
&emsp;&emsp; 根据我们使用以前的深层学习结构的背景，你会发现为什么递归神经网络是特殊的。先前我们所了解的架构在输入或训练方面是不够灵活的。它们接受固定大小的序列/矢量/图像作为输入，并产生另一个固定大小结果的作为输出。RNN架构在某种程度上是不同的，因为它们允许将一个序列作为输入进行发馈传送，并将另一个序列作为输出，或者仅在输入/输出中具有序列，如图1所示。这种灵活性对于如语言建模和情感分析的多个应用非常有用：
![image](https://github.com/computeryanjiusheng2018/infodlt/blob/master/content/chapter10/chapter_10image/ap1.JPG)<br>
图10.1：RNNs在输入或输出形状方面的柔性μ<br>
&emsp;&emsp; 这些架构背后的直觉是模仿人类处理信息的方式。在任何典型的谈话中，你对某人的话的理解完全取决于他之前说过的话，你甚至可以根据他刚才说的来预测他接下来要说什么。<br>
&emsp;&emsp; 在递归神经网络的情况下，应该遵循完全相同的过程。例如，假设你想把一个特定的词翻译成句子。不能使用传统的FNNS（反馈神经系统）来实现这一点，因为它们不能将之前单词的翻译作为我们想要翻译的当前单词的输入，并且这可能导致错误的翻译，因为该单词周围缺乏上下文相关联系的信息。<br>
递归神经网络确实能够保存关于过去的信息，并且它们具有某种循环规律，可以做到在任何给定点时将先前学习的信息用于当前预测：<br>
![image](https://github.com/computeryanjiusheng2018/infodlt/blob/master/content/chapter10/chapter_10image/ap2.JPG)<br>
图10.2：递归神经网络体系结构，它的具有循环保存过去步骤的信息能力<br>
&emsp;&emsp; 在图2中，我们有一些称为A的神经网络，它接收输入X并产生和输出H。此外，它能够通过这个循环从过去的步骤中接收信息。<br>
&emsp;&emsp; 这个循环似乎不清楚，但是如果我们使用图2的展开版本，会发现它非常简单和直观，并且RNN只是相同网络（可以是普通FNN）的重复版本，如图3所示：<br>
![image](https://github.com/computeryanjiusheng2018/infodlt/blob/master/content/chapter10/chapter_10image/ap3.JPG)<br>
图3：递归神经网络体系结构的展开版本<br>
&emsp;&emsp; 递归神经网络的这种直观体系结构及其在输入/输出形状方面的灵活性，使它们非常适合于基于序列的学习任务，例如机器翻译、语言建模、情感分析、图像字幕等。
## 递归神经系统的例子
&emsp;&emsp; 现在，我们直观地理解了递归神经网络是如何工作的，以及它在基于序列的不同有趣的示例中将如何适用，让我们仔细看看这些有趣的例子。
## 字符级语言模型
&emsp;&emsp; 语言建模是语音识别、机器翻译等应用中的一项重要任务。在这一节中，我们将尝试模仿递归神经网络的训练过程，并深入了解这些网络是如何工作的。我们将构建一个字符操作的语言模型。因此，我们将给我们的网络提供一大块文本，其目的是试图建立一个预测下一个字符的概率分布，给定前面的字符，生成类似于我们在训练过程中输入的文本。<br>
&emsp;&emsp; 例如，假设我们有一个只有四个字母的语言作为词汇-HELO。这个任务是训练一个递归神经网络上的特定输入序列的字符，如Hello。在这个特定的例子中，我们有四个训练样本：<br>
&emsp;&emsp; 1、在第一个输入字符H的前提下计算字符E的概率；<br>
&emsp;&emsp; 2、给定He的作为上文，计算字符L的概率;<br>
&emsp;&emsp; 3、根据HEL的上文计算字符L的概率；<br>
&emsp;&emsp; 4、最后，在给定上文为HELL的情况下，计算字符O的概率。<br>
&emsp;&emsp; 正如我们在前几章所了解到的，深度学习通常属于机器学习技术，它只接受实值数字作为输入。因此，我们需要某种方式转换或编码输入字符的数值形式。为此，我们将使用单热矢量编码，这是一种通过具有零向量来编码文本的方法，除了向量中的单个条目之外，向量中的单个条目，即我们试图建模的这种语言词汇表中的字符的索引（在本例中为helo）。在编码我们的训练样本之后，我们将一次将它们提供给循环神经网络类型的模型。每个给定字符循环神经类型的模型的输出值将是一个4维向量（向量的大小对应于词汇表的大小），它表示词汇表中的每个字符在给定输入字符之后成为下一个字符的概率。图4阐明了这一过程：<br>
![image](https://github.com/computeryanjiusheng2018/infodlt/blob/master/content/chapter10/chapter_10image/ap4.JPG)<br>
&emsp;&emsp; 图4_循环神经网络模型的示例，其中以单热矢量编码字符作为输入，并且输出将分布在词汇表上，该词汇表表示当前字符之后最相似的μ字符<br>
&emsp;&emsp; 如图4所示，可以看到，我们将输入序列h中的第一个字符提供给模型，并且输出是表示对下一个字符的信任度的4维向量。因此，它具有输入h之后的下一个字符的置信度为1.0，e的下一个字符的置信度为2.2，对l的下一个字符的置信度为-3.0，最终对o的下一个字符的置信度为4.1。在这个特定的例子中，我们知道正确的下一个字符将是E，基于我们的训练序列Hello。因此，在训练这种循环神经型网络时，我们的主要目标是提高e作为下一个字符的置信度，并降低其他字符的置信度。为了进行这种优化，我们将使用梯度下降和反向传播算法来更新权重，并影响神经网络，以便我们的下一个字符（e，等等）为其他三个训练示例产生更高的置信度。<br>
&emsp;&emsp; 正如所看到的，循环神经网络的输出在接下来的词汇表的所有字符上产生一个置信度分布。我们可以把这个置信度分布转换成一个概率分布，因为概率需要加到1，下一个字符概率的增加将导致其他概率的降低，。对于这个特定的修改，我们可以使用标准的SoftMax分类器进行分类到每个输出向量。<br>
&emsp;&emsp; 为了从这类网络中生成文本，我们可以将初始字符馈送给模型，并获得下一个字符的概率分布，然后从这些字符中采样并将其作为输入反馈给模型。我们要生成具有所需长度的文本，就可以通过重复多次这个过程来获得所需长度的字符序列。<br>
## 使用莎士比亚数据的语言模型
&emsp;&emsp; 从前面的例子中，我们可以通过得到模型来生成文本。但是神经网络令我们惊讶得是，它不仅会生成文本，而且还会学习训练数据的样式和结构。我们可以通过训练循环神经网络类型的模型来演示这个有趣的过程，该模型针对特定类型的文本，这些文本具有特定的结构和风格，比如以下莎士比亚的作品。<br>
&emsp;&emsp; 让我们来看一下从训练网络生成的输出：<br>
Second Senator:<br>
They are away this miseries, produced upon my soul,<br>
Breaking and strongly should be buried, when I perish The earth and thoughts of many states.<br>
&emsp;&emsp; 虽然该神经网络只知道怎样一次生成一个字符，但它生成的文本和以及内容是具有情感意义的，它们实际上具有莎士比亚作品的结构和风格。<br>
## 梯度消失问题
&emsp;&emsp; 在训练这些RNN型结构的同时，我们使用梯度下降和时间的反向传播，这给许多基于序列的学习任务带来了一些成功。但是由于梯度的性质和使用了快速训练策略，所有梯度值将明显趋近于零或消失。<br>
&emsp;&emsp; 这个过程引入了梯度消失的问题，许多研究者会陷入其中。在本章的后面，我们将讨论研究人员如何处理这些问题，并产生普通的递归神经网络的变异来克服这个问题：<br>
![image](https://github.com/computeryanjiusheng2018/infodlt/blob/master/content/chapter10/chapter_10image/ap5.JPG)<br>
图5：梯度消失问题
## 长期依赖问题
&emsp;&emsp; 研究人员所面临的另一个挑战性问题是人们在文本中可以找到的长期依赖性。例如，如果像我以前在法国生活一样，我学会了说话……顺序中的下一个显而易见的词是法语。<br>
&emsp;&emsp; 在这种情况下，普通的递归神经网络将能够处理它，因为它具有短期依赖性，如图6所示：<br>
![image](https://github.com/computeryanjiusheng2018/infodlt/blob/master/content/chapter10/chapter_10image/ap6.JPG)<br>
图6：文本的短期依赖<br>
&emsp;&emsp; 另一个例子是，如果有人说我以前住在法国…然后他开始描述那里的生活，最后我学会了说法语。因此，为了让模型预测他/她在序列末尾所学的语言，模型需要一些关于早期单词live和法国的信息。如果模型无法跟踪文本中的长期依赖性，那么它将无法处理此类情况：
![image](https://github.com/computeryanjiusheng2018/infodlt/blob/master/content/chapter10/chapter_10image/ap7.JPG)<br>
图7：文本中长期依赖性的挑战<br>
&emsp;&emsp; 为了处理文本中逐渐消失的梯度和长期依赖性，研究人员引入了一种称为长短期记忆网络（LSTM）的普通循环神经网络。
## 长短期记忆网络
&emsp;&emsp; 长短期记忆网络是循环神经网络的一种变形，用于帮助解决文本学习中的长期依赖关系。长短期记忆网络最初由Hochreiter &Schmidhuber(1997)引入，已经许多研究者对其进行了研究，并在许多领域产生了有趣的结果。<br>
&emsp;&emsp; 这些类型的体系结构将能够处理文本中的长期依赖问题，主要是因为它们的内部体系结构决定的。<br>
&emsp;&emsp; 长短期记忆网络与普通循环神经网络类似，因为随着时间推移，它有一个重复模块，但是这个重复模块的内部结构与普通循环神经网络不同。它包含了更多的被遗忘和已经更新信息的层次：<br>
![image](https://github.com/computeryanjiusheng2018/infodlt/blob/master/content/chapter10/chapter_10image/ap8.JPG)<br>
图8：包含单个层的标准循环神经网络中的重复模块<br>
&emsp;&emsp; 如前所述，普通递归神经网络具有单个神经网络层，但是长短期记忆网络具有以特殊方式相互作用的四个不同层。这种特殊的交互使得长短期记忆网络在很多领域都能很好地工作，我们将在构建语言模型示例时看到：<br>
![image](https://github.com/computeryanjiusheng2018/infodlt/blob/master/content/chapter10/chapter_10image/ap9.JPG)<br>
图9：包含四个相互作用层的长短期记忆网络中的重复模块<br>
&emsp;&emsp; 关数学细节以及四个层如何实现交互的更多细节，可以看一下这个有趣的教程：
`http://colah. github.io/posts/2Ol5–O8–Understanding–L3TMs/`
## 长短期记忆网络的工作原理是什么？
&emsp;&emsp; 在我们的普通的长短期记忆网络架构中，第一步是确定哪些信息不是必需的，并且通过丢弃这些信息来为更重要的信息留出更多的空间。为此，我们有一个称为遗忘栅极层的层，它查看以前的输出ht-1和当前输入xt，并决定要丢弃哪些信息。<br>
有&emsp;&emsp; 长短期记忆网络体系结构的下一步是决定哪些信息值得保存，并存储在单元格中。这是通过两个步骤完成的：
1、一个称为输入控制门的层，它决定单元的前一个状态的值需要更新。<br>
2、第二步是生成一组新的候选值，这些值将被添加到单元格中。<br>
&emsp;&emsp; 最后，我们需要决定长短期记忆网络单元将输出什么。此输出将基于我们的单元格状态，但将是经过过滤的版本。
## 语言模型的实现
&emsp;&emsp; 在本节中，我们将构建一个通过字符操作的语言模型。对于这个模型的实现，我们将使用安娜·卡列尼娜的小说作为实例，看看网络将如何学习实现文本的结构和风格：
![image](https://github.com/computeryanjiusheng2018/infodlt/blob/master/content/chapter10/chapter_10image/ap10.JPG)<br>
图10：字符级循环神经网络的一般体系结构<br>
&emsp;&emsp; 这个网络结构是基于Andrej Karpathy's在循环神经网络系统实现的。<br>
&emsp;&emsp; 我们将构建一个基于安娜·卡列尼娜小说的人物的循环神经网络，它能够根据书中的文本生成新的文本。将找到包含在该实现的现实中的.txt文件。让我们先为这个字符级模型的实现导入必要的库：
```import numpy as np import tensorflow as tf
from collections import namedtuple
```
&emsp;&emsp; 首先，我们需要通过加载数据集并将其转换成整数来准备训练数据集。因此，我们将字符转换成整数编码，然后将它们编码为整数，这使得它作为模型的输入变量变得简单且易于使用：
```
#reading the Anna Karenina novel text file with open('Anna_Karenina.txt', 'r') as f:
textlines=f.read()
#Building the vocan and encoding the characters as integers language_vocab = set(textlines)
vocab_to_integer = (char: j for j, char in enumerate(language_vocab)} integer_to_vocab = dict(enumerate(language_vocab))
encoded_vocab = np.array([vocab_to_integer[char] for char in textlines], dtype=np.int32)
```
&emsp;&emsp; 那么，让我们来看看Anna Karenina文本中的前200个字符：
```
extlines[:2OO] Output:
"Chapter l\n\n\nHappy families are all alike; every unhappy family is unhappy in its own\nway.\n\nEverything was in confusion in the Oblonskys' house. The wife had\ndiscovered that the husband was carrying on"
```
&emsp;&emsp; 我们还把字符转换成一种方便的形式--整数。那么，让我们来看看字符的编码版本：
```
encoded_vocab[:2OO] Output:
array([7O, 34, 54, 29, 24, l9, 76, 45, 2, 79, 79, 79, 69, 54, 29, 29, 49,
45, 66, 54, 39, l5, 44, l5, l9, l2, 45, 54, 76, l9, 45, 54, 44, 44,
45, 54, 44, l5, 27, l9, 58, 45, l9, 3O, l9, 76, 49, 45, 59, 56, 34,
54,	29,	29,	49,	45,	66,	54,	39,	l5,	44,	49,	45,	l5,	l2,	45,	59,	56,
34,	54,	29,	29,	49,	45,	l5,	56,	45,	l5,	24,	l2,	45,	ll,	35,	56,	79,
35,	54,	49,	53,	79,	79,	36,	3O,	l9,	76,	49,	24,	34,	l5,	56,	l6,	45,
35,	54,	l2,	45,	l5,	56,	45,	3l,	ll,	56,	66,	59,	l2,	l5,	ll,	56,	45,
l5, 56, 45, 24, 34, l9, 45, l, 82, 44, ll, 56, l2, 27, 49, l2, 37,
45, 34, ll, 59, l2, l9, 53, 45, 2l, 34, l9, 45, 35, l5, 66, l9, 45,
34, 54, 64, 79, 64, l5, l2, 3l, ll, 3O, l9, 76, l9, 64, 45, 24, 34,
54, 24, 45, 24, 34, l9, 45, 34, 59, l2, 82, 54, 56, 64, 45, 35, 54,
l2, 45, 3l, 54, 76, 76, 49, l5, 56, l6, 45, ll, 56], dtype=int32)
```
&emsp;&emsp; 由于网络处理单个字符的时候，它类似于一个分类问题，在这个问题中我们试图从前一个文本中预测下一个字符。下面是我们的网络需要选择的类。因此，我们将一次向模型反馈传送一个字符，并且模型将通过对可能接下来出现的字符数(词汇)产生概率分布来预测下一个字符，该概率分布应该是需要从以下几个类中挑选出来的类：
```
len(language_vocab) 
Output:
83
```
&emsp;&emsp; 由于我们将使用随机梯度法下降来训练我们的模型，我们需要将我们的数据转换成训练多个批次。
## 生成小批量训练集
&emsp;&emsp; 在这一节中，我们将我们的数据分成小批次用于训练模型。因此，小批次训练集将由期望序列数的有序序列步骤组成。那么，让我们看看图11中的一个可视化示例：<br>
![image](https://github.com/computeryanjiusheng2018/infodlt/blob/master/content/chapter10/chapter_10image/ap11.JPG)<br>
图11说明批次和序列的例子<br>
&emsp;&emsp; 所以，现在我们需要定义一个函数，它将迭代经过编码的文本并生成批处理训练集。在这个函数中，我们将使用一个非常好的Python机制，叫做yield。<br>
&emsp;&emsp; 一个典型的批次将具有N×M个字符，其中N是序列的数目，M是序列步长的数目。为了获得数据集中可能批的数量，我们可以简单地将数据的长度除以所需的批大小，在获得该数量的可能批之后，我们可以知道需要操作的每个批中应该有多少字符。<br>
&emsp;&emsp; 之后，我们需要将数据集分割成期望数量的序列（N）。我们可以使用数组重塑（尺寸）。我们知道我们需要N个序列（num_seqs被使用，遵循代码），让我们做第一个维度的大小。对于第二个维度，可以使用-1作为占位符；它将填充数组，并提供适当的数据。在此之后，应该有一个N×（M`*`K)的数组，其中K是批数。<br>
&emsp;&emsp; 现在我们有了这个数组，我们可以遍历它来获得训练批次，每个批次都有N×M字符。对于每一个后续批次，窗口通过num_steps移动。最后，我们还希望为我们的输入和输出数组创建模型输入。创建输出值的这一步骤非常简单；记住目标是在一个字符上移位的输入。通常会看到第一个输入字符作为最后一个目标字符，所以像这样：








学号|姓名|专业
-|-|-
201802110485|李忠|计算机应用技术
