# 目标检测-卷积神经网络的迁移学习<br>
## 如何在一个环境迁移到另一个具有相似特征的环境中加以应用。
                                                       – E. L. Thorndike, R. S. Woodworth (1991)<br>
迁移学习（TL）是数据科学中的一个研究问题，主要涉及在解决特定任务中获得的知识的持续性，并利用所获得的知识来解决另一个不同但相似的任务。在这一章中，我们将演示迁移学习。数据科学领域中使用的一种现代实践和共同主题。这里的想法是如何从具有非常大的数据集的领域获得对数据集较小的领域的帮助。最后，我们将重温CIFAR-10的目标检测示例，并尝试通过迁移学习减少训练时间和性能误差。
本章将讨论以下主题：<br>
     •迁移学习<br>
     •CIFAR-10目标检测回顾<br>
# 迁移学习<br>
深度学习的架构是依赖于数据的，并且在训练集中只有少量样本不会使我们从中获益。迁移学习通过将已学习或获得的知识/表示为未解决具有大数据集的任务转移到具有小数据集的另一个不同但相似的任务来解决这个问题。迁移学习不仅适用于小训练集的情况，而且我们可以使用它使训练过程更快。从零开始训练大型的深度学习架构有时可能非常慢，因为我们在这些架构中有数百万的权重需要学习。取而代之的是，人们可以通过微调学习到的权重来利用迁移学习，就像他/她试图解决的问题一样。<br>
## 迁移学习背后的直觉<br>
让我们用下面的师生类比建立迁移学习背后的直觉。一位教师在他所教的教学模块中有多年的经验。另一方面，学生从老师给出的讲座中得到一个紧凑的主题概述。所以你可以说老师正在把他们的知识以简洁而紧凑的方式传递给学生。<br>
教师和学生的类比同样适用于我们在深度学习或一般的神经网络中传递知识的情况。因此，我们的模型从数据中学习一些表示，这是由网络的权值来表示的。这些学习的表征/特征（权重）可以被转移到另一个不同但相似的任务。将学习的权重转移到另一个任务的过程将减少对深度学习架构的巨大数据集收敛的需要，与从头开始训练模型相比，并且也将减少模型适应新数据集所需的时间。<br>
现在，深度学习被广泛使用，但通常大多数人在训练深度学习体系结构时使用迁移学习；很少有人从头开始训练深度学习体系结构，因为大多数时候很少有足够的数据集来进行深层次的学习。因此，在像IMANET这样的大型数据集上使用预先训练的模型是非常普遍的，它有大约120万张图像，并将其应用到新的任务中。我们可以使用预先训练好的模型的权重作为特征提取器，或者我们可以用它初始化我们的体系结构，然后根据新的任务对其进行微调。现在使用迁移学习有三种主要方案：<br>
## 1.使用卷积网络作为固定特征提取器<br>
在此场景中，诸如ImageNet之类的大型数据集上使用经过预训练的卷积模型，并使用它来解决问题。例如，在ImageNet上预先训练的卷积模型将有一个完全连接的层，该层具有ImageNet所具有的1000个类别的输出分数。因此，需要删除这个层，因为对IMANET的类不再感兴趣。然后，将所有其他层视为特征提取器。一旦使用预先训练的模型提取了特征，就可以将这些特征提供给任何线性分类器，例如Softmax分类器，甚至线性SVM（SVM支持向量机：是一种二类分类模型，他的基本模型是定义在特征空间上的间隔最大的线性分类器。 间隔最大使它有别于感知机；支持向量机还包括核技巧，这使它成为实质上的线性分类器。支持向量机的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题，支持向量机的学习算法是求解凸二次规划的最优化算法）。<br>
## 2.卷积神经网络的微调<br>
第二种方案涉及第一种方案，但是要额外使用反向传播来微调新任务上的预先训练的权重。通常，人们把大部分的层固定下来，只对网络的顶端进行微调。尝试微调整个网络，但是大多数层可能会导致过度拟合。所以，你可能只关注微调那些与图像的语义层次特征有关的层。对早期的层固定下来的层的直觉是它们包含通用的或低级的特性，这些特性在大多数成像任务中都是通用的，比如角、边等等。如果要引入新类，那么对网络的较高层或顶端层进行微调将是非常有用的，这些类在模型预先训练的原始数据集中是不存在的。<br>
