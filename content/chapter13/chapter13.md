
# &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;第13章 自动编码器-特征提取和去噪
&emsp;&emsp;自动编码器网络现在是广泛使用的深度学习架构之一。 它主要用于高效解码任务的无监督学习。 它还可以通过学习特定数据集的编码或表示用于来降低维数。 在本章中使用自动编码器，我们将展示如何通过构建具有相同尺寸但噪声较小的另一个数据集来对数据集进行去噪。 为了在实践中使用这个概念，我们将从MNIST数据集中提取重要特征，并试着看看如何通过这个显着增强性能。<br>
&emsp;&emsp;&emsp;本章将介绍以下主题：<br>
&emsp;&emsp;&emsp;&emsp;---自动编码器简介<br>
&emsp;&emsp;&emsp;&emsp;---自动编码器实例<br>
&emsp;&emsp;&emsp;&emsp;---自动编码器架构<br>
&emsp;&emsp;&emsp;&emsp;---压缩MNIST数据集<br>
&emsp;&emsp;&emsp;&emsp;---卷积自动编码器<br>
&emsp;&emsp;&emsp;&emsp;---去噪自动编码器<br>
&emsp;&emsp;&emsp;&emsp;---自动编码器的应用<br>
## 自动编码器简介<br>
&emsp;&emsp;自动编码器是另一种可用于许多有趣任务的深度学习架构，但它也可以被视为香草前馈神经网络的变体，其中输出具有与输入相同的维数。如图1所示，自动编码器的工作方式是将数据样本（x1，...，x6）提供给网络。它将尝试在L2层中学习此数据的较低表示，您可以将其称为以较低表示形式对数据集进行编码的方法。然后，网络的第二部分（可称为解码器）负责构造此表示的输出。您可以将网络从输入数据中学习的中间较低表示视为其压缩版本。<br>
&emsp;&emsp;与我们迄今为止看到的所有其他深度学习架构没有太大差别，自动编码器使用反向传播算法。自动编码器神经网络是一种适用的无监督学习算法反向传播，将目标值设置为等于输入：<br>
![image001](https://github.com/computeryanjiusheng2018/infodlt/blob/master/content/chapter13/chapter13_image/image001.png)<br>
图1 通用自动编码器架构<br>
## 自动编码器示例<br>
&emsp;&emsp;在本章中，我们将演示使用MNIST数据集的自动编码器的不同变体的一些示例。作为具体示例，假设输入x是来自28×28图像（784像素）的像素强度值; 所以输入数据样本的数量是n = 784。 L2层中有s2 = 392个隐藏单位。 并且由于输出将与输入数据样本具有相同的维度，因此y z R784。输入层神经元数为784，中层神经元数为392；因此，网络将是更低的表示，它是输出的压缩版本。然后，网络将把输入a(L2)z R392的压缩较低表示馈送给网络的第二部分，后者将努力从这个压缩版本重建输入像素784。<br>
&emsp;&emsp;自动编码器依赖于这样的事实，即由图像像素表示的输入样本将以某种方式相关，然后它将使用这个事实来重建它们。因此，自动编码器有点类似于降维技术，因为它们还学习了输入数据的较低表示。<br>
&emsp;&emsp;&emsp;综上所述，一个典型的自动编码器将由三部分组成：<br>
&emsp;&emsp;&emsp;&emsp;1.编码器部分，其负责将输入压缩为较低表示<br>
&emsp;&emsp;&emsp;&emsp;2.代码，这是编码器的中间结果<br>
&emsp;&emsp;&emsp;&emsp;3.解码器，负责使用该代码重构原始输入<br>
下图显示了一个典型的自动编码器的三个主要组成部分：
